# Local LLM to help answer user questions from provided text/PDF/doc files

1.  ** Features ** 
  -   **Text Analysis**: The model can analyze text from various sources, including PDFs, Word Docs, CSVs, plain text files
  -   
Supports PDFs, Word Docs, CSVs, and plain text files.

Automatically builds and saves a ChromaDB vector store and leveraging Langchain features too. 

Streamlit-based UI for interactive document Q&A.

2. ** How to run ** 
   run using streamlit run app.py after creating the folder structure as given below (step#3)

3. . ** Folder Structure **
### ğŸ“ Folder Structure
# .
# â”œâ”€â”€ app.py
# â”œâ”€â”€ config.py
# â”œâ”€â”€ loader.py
# â”œâ”€â”€ rag_chain.py
# â”œâ”€â”€ documents/                <-- Optional static source
# â”œâ”€â”€ rag_vectorstore/         <-- Auto-generated Chroma vector DB
# â””â”€â”€ temp_uploads/            <-- Temporary folder for uploaded files

